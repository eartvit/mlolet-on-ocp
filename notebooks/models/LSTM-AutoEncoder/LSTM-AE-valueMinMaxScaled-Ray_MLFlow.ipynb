{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fd0468-6aec-44b6-b25c-41f4e33d2161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "os.environ[\"RAY_IGNORE_UNHANDLED_ERRORS\"] = \"1\"\n",
    "os.environ[\"TUNE_DISABLE_STRICT_METRIC_CHECKING\"] = \"1\"\n",
    "os.environ[\"RAY_memory_monitor_refresh_ms\"] = \"0\" # do not kill raylet if low on memmory\n",
    "os.environ[\"RAY_TASK_MAX_RETRIES\"] = \"2\"\n",
    "\n",
    "os.environ[\"TUNE_PLACEMENT_GROUP_AUTO_DISABLED\"] = \"1\" #neded only when running local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa9295d-1649-44d5-85ef-1d53b8bd3b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import TimeDistributed, LSTM, Dense, Dropout, Flatten, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c4d6c1-5e6c-4cfe-9a01-f1c81e433c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import train, tune, air\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a90ec4-dca9-446c-bba8-757be0778772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d263b45-a0c9-4f3c-99f6-377b799629bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "from ray.air.integrations.mlflow import setup_mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ab2a1e-8bf8-41ac-b660-ce8c3fc0a3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad016c2b-308b-4062-9b6d-2fe45b53436a",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelTypeName = \"LSTM_AE_valueMinMaxScaled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481ead78-6a6e-405c-b8b3-64e5622226c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataColumnName = 'valueMinMaxScaled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5d1e63-efa7-4b42-86ab-749f78229cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLFLOW_URI='http://localhost:8080/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4822286-4fdd-400c-9f21-a031062e5973",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_space={\n",
    "    \"n_epochs\": tune.choice([200]),\n",
    "    \"n_layers\": tune.choice([1, 3, 5, 10]),\n",
    "    \"n_dense_layers\": tune.choice([1, 3, 5]), \n",
    "    \"learning_rate\": tune.choice([0.003, 0.001]),\n",
    "    \"activation\": tune.choice(['tanh', 'relu']), \n",
    "    \"n_neurons\": tune.choice([32, 64, 96, 128, 256]),\n",
    "    \"T\": tune.choice([5, 10, 15, 20]),\n",
    "    \"dense_dp\": tune.choice([0.2, 0.3, 0.4]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14e90dc-2eb7-430d-b456-e34e7c1db2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run '../lib/utils_anomaly_detection.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94211d75-d898-4b48-bf64-bdbca6b7aa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b17d5c-4253-4bf5-b0a0-7544492d4697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "data = pd.read_csv('../../data/rq_1-3_train_test/2024-01-15_10-51-45__2024-01-15_14-26-45_load-gen-msg-w-spikes-10s-rate.csv', \n",
    "                   index_col=['EventDateTime'], parse_dates=['EventDateTime'])\n",
    "dataLatency = pd.read_csv('../../data/rq_1-3_train_test/2024-01-15_10-51-45__2024-01-15_14-26-45_load-gen-avg-latency-10s-rate.csv', \n",
    "                          index_col='EventDateTime', parse_dates=['EventDateTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa35217-0237-489a-98d9-0fa9b0320262",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec615467-1eda-499e-a6b3-2a27d4cd8c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run '../lib/prepareDataSet.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6173b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_percentage_error, mean_squared_error, mean_absolute_error\n",
    "from keras.models import load_model, save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde92128-af65-4923-81c0-467159f55ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827103c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " This function uses one Tensor formatted input from the X_test dataset in order to predict ahead by a number\n",
    " of given steps. Then it readjusts by using another true value from X_test (as the nex index) before starting\n",
    " a new prediction cycle\n",
    "\"\"\"\n",
    "def generate_nsteps_forecast(x_test, y_test, nn_model, enc_model, pred_ahead):\n",
    "    max_len = x_test.shape[0]\n",
    "    y_pred = []\n",
    "    last_x = x_test[0]\n",
    "    dim_shape = last_x.shape[1]\n",
    "    index = 0\n",
    "    ae_errors_at_p = []\n",
    "    se_errors_at_p = []    \n",
    "    while len(y_pred) < max_len:\n",
    "        sequence = 0\n",
    "        while sequence < pred_ahead:\n",
    "            try:\n",
    "                x_crt_input = last_x.reshape(1, -1, dim_shape)\n",
    "                p_vector = nn_model.predict(x_crt_input, verbose=0)\n",
    "                p = p_vector[0,0] # 1x1 array -> scalar\n",
    "            except:\n",
    "                print(f'Prediction error for x={x_crt_input} at sequence={sequence} for start index={index} when pred_ahead={pred_ahead}')\n",
    "                print(f'Model config was:{nn_model.get_config()}')\n",
    "                p = 0\n",
    "                \n",
    "            # update the predictions list\n",
    "            y_pred.append(p)\n",
    "\n",
    "            #prepare new_x cointaing prediction p for encoding\n",
    "            new_x = last_x[:,0] # the first column contains the original input x from X_train\n",
    "            new_x = np.roll(new_x, -1)\n",
    "            new_x[-1] = p\n",
    "            \n",
    "            try:\n",
    "                x_vector_ = np.concatenate([new_x.reshape(1, -1, D), \n",
    "                                 enc_model.predict(new_x.reshape(1, -1, D), verbose=0)], \n",
    "                                axis = -1)\n",
    "                x_ = x_vector_[0][0] # x_ is 3D we need only the last dimension as a row\n",
    "            except:\n",
    "                x_ = 0\n",
    "\n",
    "            # make the new input\n",
    "            last_x = np.roll(last_x, -1)\n",
    "            last_x[-1] = x_\n",
    "            \n",
    "            # increase index for the next run\n",
    "            sequence += 1\n",
    "\n",
    "        index += sequence\n",
    "        if index < max_len:\n",
    "            last_x = x_test[index]\n",
    "            ae_errors_at_p.append(np.absolute(p - y_test[index-1]))\n",
    "            se_errors_at_p.append(np.square(p - y_test[index-1]))\n",
    "            #print(f\"Arrived at index = {index} of {max_len} with value X={last_x}\")\n",
    "    \n",
    "    if len(y_pred) > max_len:\n",
    "        # predicted too much, cutoff the tail\n",
    "        y_pred = y_pred[0:max_len]\n",
    "        \n",
    "    y_pred_array = np.array(y_pred)\n",
    "    avg_ae = np.mean(ae_errors_at_p)\n",
    "    avg_se = np.mean(se_errors_at_p)\n",
    "    return (y_pred_array, avg_ae, avg_se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7ac612-f1fa-43bc-bee1-ff4558960b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_autoencoder(config, X_train, Y_train, X_test, Y_test, autoencoder_name):\n",
    "    inputs_ae = Input(shape=(config['T'],D))\n",
    "    encoded_ae = LSTM(config['n_neurons'], return_sequences=True, activation='relu')(inputs_ae, training=True)\n",
    "    decoded_ae = LSTM(config['n_neurons'], return_sequences=True, activation='relu')(encoded_ae, training=True)\n",
    "    out_ae = TimeDistributed(Dense(1))(decoded_ae)\n",
    "\n",
    "    sequence_autoencoder = Model(inputs_ae, out_ae)\n",
    "    sequence_autoencoder.compile(loss='mse', optimizer=Adam(learning_rate=config['learning_rate']))\n",
    "\n",
    "    es_ae = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=5, restore_best_weights=True)\n",
    "    mc_ae = ModelCheckpoint(autoencoder_name, monitor='val_loss', mode='auto', save_best_only=True)\n",
    "    \n",
    "    results = sequence_autoencoder.fit(X_train, Y_train, epochs=config['n_epochs'], validation_data=(X_test, Y_test), \n",
    "                                       callbacks=[es_ae, mc_ae], verbose = 0)\n",
    "    \n",
    "    encoder = Model(inputs_ae, encoded_ae)\n",
    "    \n",
    "    return encoder, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d54708-62b9-4958-8580-0b5696f293e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_forecaster(config, x_train_shape):\n",
    "    \n",
    "    forecaster = Sequential()\n",
    "    forecaster.add(Input(shape=x_train_shape))\n",
    "    for i in range(config[\"n_layers\"] - 1):\n",
    "        forecaster.add(LSTM(config[\"n_neurons\"], activation=config['activation'],return_sequences=True))\n",
    "        \n",
    "    forecaster.add(LSTM(config[\"n_neurons\"], activation=config['activation'],return_sequences=False))\n",
    "    \n",
    "    for i in range(config[\"n_dense_layers\"]-1):\n",
    "        forecaster.add(Dense(config[\"n_neurons\"], activation=\"relu\"))\n",
    "        forecaster.add(Dropout(rate=config[\"dense_dp\"]))\n",
    "\n",
    "    forecaster.add(Dense(1))\n",
    "            \n",
    "    forecaster.compile(loss='mse', optimizer=Adam(learning_rate=config['learning_rate']))\n",
    "    \n",
    "    return forecaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae32852-2dda-40c0-8964-70071a3c63cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    \n",
    "    n_neurons = config['n_neurons']\n",
    "    n_epochs = config['n_epochs']\n",
    "    learning_rate = config['learning_rate']\n",
    "    T = config['T']\n",
    "\n",
    "    model_exp =  f'{modelTypeName}'\n",
    "    UUID = uuid.uuid4().hex\n",
    "\n",
    "    mlflow_exp_name = f'{modelTypeName}-{UUID}-T_{T}-LY_{config[\"n_layers\"]}-DLY_{config[\"n_dense_layers\"]}-NN_{n_neurons}'\n",
    "\n",
    "    import mlflow\n",
    "\n",
    "    mlflow.set_tracking_uri(MLFLOW_URI)\n",
    "    mlflow.set_registry_uri(MLFLOW_URI)\n",
    "    \n",
    "    X_train, Y_train, X_test, Y_test = prepare_dataset(dataFrame[dataColumnName], T)\n",
    "    \n",
    "    model_name = modelTypeName + \"-T_\" + str(T) + \"-LY_\" + str(config['n_layers']) + \"-DLY_\" + str(config['n_dense_layers']) +\\\n",
    "                 \"-NN_\" + str(n_neurons) + \"-LR_\" + str(learning_rate) + \"-epochs_\" + str(n_epochs) +\"-\" + UUID + \".keras\"\n",
    "    autoencoder_name = \"ae_\"+ modelTypeName + \"-T_\" + str(T) + \"-LY_\" + str(config['n_layers']) + \"-DLY_\" + str(config['n_dense_layers']) +\\\n",
    "                 \"-NN_\" + str(n_neurons) + \"-LR_\" + str(learning_rate) + \"-epochs_\" + str(n_epochs) +\"-\" + UUID + \".keras\"\n",
    "\n",
    "    experiment_id = mlflow.create_experiment(mlflow_exp_name)\n",
    "    with mlflow.start_run(run_name=mlflow_exp_name, experiment_id=experiment_id) as mlflowrun:\n",
    "        run_id = mlflowrun.info.run_id\n",
    "    \n",
    "        #autoencode\n",
    "        autoencoder, training_results = create_autoencoder(config, X_train, Y_train, X_test, Y_test, autoencoder_name)\n",
    "        try:\n",
    "            y_predict_ae = autoencoder.predict(X_test, verbose=0)\n",
    "            signature_ae = infer_signature(X_test, y_predict_ae)\n",
    "\n",
    "            mlflow.tensorflow.log_model(autoencoder, autoencoder_name, \n",
    "                                            signature = signature_ae,\n",
    "                                            #input_example=X_train[0].reshape(1, -1, 1), \n",
    "                                            registered_model_name = autoencoder_name)\n",
    "            #save_model(encoder, autoencoder_name)\n",
    "\n",
    "        except:\n",
    "            print(f'Ray-MLFlow: Could not save model {autoencoder_name}')\n",
    "        \n",
    "        #forecaster\n",
    "        X_train_ = np.concatenate([X_train, autoencoder.predict(X_train, verbose=0)], axis = -1)\n",
    "        X_test_ = np.concatenate([X_test, autoencoder.predict(X_test, verbose=0)], axis = -1)\n",
    "        \n",
    "        x_train_shape = X_train_.shape[1:]\n",
    "        nn_model = create_forecaster(config, x_train_shape)\n",
    "        es_m = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=5, restore_best_weights=True)\n",
    "        mc_m = ModelCheckpoint(model_name, monitor='val_loss', mode='min', save_best_only=True)    \n",
    "        train_results = nn_model.fit(X_train_, Y_train, epochs=n_epochs, validation_data=(X_test_, Y_test), \n",
    "                            callbacks=[es_m, mc_m], verbose=0)\n",
    "            \n",
    "        #evaluate and print results of forecaster\n",
    "        try:\n",
    "            saved_model = load_model(model_name)\n",
    "        except:\n",
    "            saved_model = nn_model\n",
    "            save_model(nn_model, model_name)\n",
    "\n",
    "        # Reconstruct signal using the train data and get the AE and SE errors to obtain the 3-sigma thresholds\n",
    "        y_pred_train = saved_model.predict(X_train_, verbose=0)\n",
    "\n",
    "        errors_ae_train = calculate_absolute_prediction_errors(Y_train, y_pred_train)\n",
    "        lo_3sigma_ae, up_3sigma_ae = calculate_3sigma_threshold(errors_ae_train)\n",
    "        anomalies_ae_train = calculate_3sigma_anomalies(errors_ae_train, lo_3sigma_ae, up_3sigma_ae)\n",
    "        \n",
    "        errors_se_train = calculate_squared_prediction_errors(Y_train, y_pred_train)\n",
    "        lo_3sigma_se, up_3sigma_se = calculate_3sigma_threshold(errors_se_train)\n",
    "        anomalies_se_train = calculate_3sigma_anomalies(errors_se_train, lo_3sigma_se, up_3sigma_se)\n",
    "    \n",
    "        three_sigma_thresholds = {}\n",
    "        three_sigma_thresholds['lo_3sigma_ae'] = lo_3sigma_ae\n",
    "        three_sigma_thresholds['up_3sigma_ae'] = up_3sigma_ae\n",
    "        three_sigma_thresholds['lo_3sigma_se'] = lo_3sigma_se\n",
    "        three_sigma_thresholds['up_3sigma_se'] = up_3sigma_se\n",
    "        \n",
    "        # Test spike detection using 3-sigma rule on the test data        \n",
    "        y_predict = saved_model.predict(X_test_, verbose=0)\n",
    "        \n",
    "        errors_ae = calculate_absolute_prediction_errors(Y_test, y_predict)\n",
    "        anomalies_ae = calculate_3sigma_anomalies(errors_ae, lo_3sigma_ae, up_3sigma_ae)\n",
    "        errors_se = calculate_squared_prediction_errors(Y_test, y_predict)\n",
    "        anomalies_se = calculate_3sigma_anomalies(errors_se, lo_3sigma_se, up_3sigma_se)\n",
    "\n",
    "        try:\n",
    "            r2 = r2_score(Y_test, y_predict)\n",
    "        except:\n",
    "            r2 = 110\n",
    "        if np.isnan(r2):\n",
    "            r2 = 110\n",
    "\n",
    "        try:\n",
    "            mae = mean_absolute_error(Y_test, y_predict)\n",
    "        except:\n",
    "            mae = 100\n",
    "        if np.isnan(mae):\n",
    "            mae = 100\n",
    "\n",
    "        try:\n",
    "            mape = mean_absolute_percentage_error(Y_test, y_predict)\n",
    "        except:\n",
    "            mape = 100\n",
    "        if np.isnan(mape):\n",
    "            mape = 100\n",
    "\n",
    "        try:\n",
    "            mse = mean_squared_error(Y_test, y_predict)\n",
    "        except:\n",
    "            mse = 100\n",
    "        if np.isnan(mse):\n",
    "            mse = 100\n",
    "\n",
    "        try:\n",
    "            pcc = np.corrcoef(Y_test, y_predict.flatten())[0,1]\n",
    "        except:\n",
    "            pcc = 100\n",
    "        if np.isnan(pcc):\n",
    "            pcc = 100\n",
    "\n",
    "\n",
    "        fig = plt.figure(figsize=(20,15))\n",
    "        plt.title(\"Spikes Train Data\")\n",
    "        plt.plot(Y_train,label=\"Original Data\", alpha=0.6, c='gray')\n",
    "        plt.plot(y_pred_train,label=\"Predict 1-step Forecast\", alpha=0.6, c='red', linewidth=3)\n",
    "        plt.scatter(np.where(anomalies_ae_train==True), y_pred_train[np.where(anomalies_ae_train==True)], \n",
    "                    alpha=0.8, color='green', s=350, label=\"3-Sigma Anomalies AE\")\n",
    "        plt.scatter(np.where(anomalies_se_train==True), y_pred_train[np.where(anomalies_se_train==True)], \n",
    "                    alpha=0.8, color='magenta', s=150, label=\"3-Sigma Anomalies SE\")\n",
    "        plt.legend()\n",
    "        figName = f\"Y_train_spikes-T_{T}.png\"\n",
    "        mlflow.log_figure(fig, figName)\n",
    "        #plt.savefig(figName, transparent=False)\n",
    "        fig.clf()\n",
    "        plt.close()\n",
    "    \n",
    "        fig = plt.figure(figsize=(20,15))\n",
    "        plt.title(\"Spikes Test Data T=\" + str(T) + \" with predict 1 on \"+ str(model_exp) +\": NN=\" + str(n_neurons) + \" epochs=\" + str(n_epochs) +\n",
    "                  \" lr=\" + str(learning_rate))\n",
    "        plt.plot(y_predict,label=\"Predict 1-step Forecast\", alpha=0.6, c='red', linewidth=3)\n",
    "        plt.plot(Y_test,label=\"Original Data\", alpha=0.6, c='black')\n",
    "        plt.scatter(np.where(anomalies_ae==True), y_predict[np.where(anomalies_ae==True)], \n",
    "                    alpha=0.8, color='green', s=350, label=\"3-Sigma Anomalies AE\")\n",
    "        plt.scatter(np.where(anomalies_se==True), y_predict[np.where(anomalies_se==True)], \n",
    "                    alpha=0.8, color='magenta', s=150, label = \"3-Sigma Anomalies SE\")\n",
    "        plt.legend()    \n",
    "        figName = f\"Y_predict-1-step-spikes-T_{T}.png\"\n",
    "        mlflow.log_figure(fig, figName)\n",
    "        #plt.savefig(figName, transparent=False)\n",
    "        fig.clf()\n",
    "        plt.close()\n",
    "            \n",
    "        try:\n",
    "            signature = infer_signature(X_test_, y_predict)\n",
    "    \n",
    "            mlflow.tensorflow.log_model(nn_model, model_name, \n",
    "                                            signature = signature,\n",
    "                                            registered_model_name = model_name)\n",
    "        except:\n",
    "            print(f'Ray-MLFlow: Could not save model {model_name}')\n",
    "            \n",
    "        try:\n",
    "            mlflow.log_dict(three_sigma_thresholds, \"three_sigma_thresholds.json\")\n",
    "            mlflow.log_param(\"n_layer_size\", n_neurons)\n",
    "            mlflow.log_param(\"n_layers\", config['n_layers'])\n",
    "            mlflow.log_param(\"n_dense_layers\", config['n_dense_layers'])\n",
    "            mlflow.log_param(\"activation_fn\", config['activation'])\n",
    "            mlflow.log_param(\"epochs\", n_epochs)\n",
    "            mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "            mlflow.log_param(\"optimizer\", \"adam\")\n",
    "            mlflow.log_param(\"time_window\", config['T'])\n",
    "            mlflow.log_param(\"dense_dp\", config['dense_dp'])\n",
    "            mlflow.log_param(\"model_exp\", model_exp)\n",
    "            \n",
    "            mlflow.log_metric(\"mae\", mae)\n",
    "            mlflow.log_metric(\"mse\", mse)\n",
    "            mlflow.log_metric(\"mape\", mape)\n",
    "            mlflow.log_metric(\"r2_score\", r2)\n",
    "            mlflow.log_metric(\"pearson_corr_coef\", pcc)\n",
    "            \n",
    "        except:\n",
    "            exception_param_metric_dict = {}\n",
    "            log_metric_dict = {\n",
    "                'r2_score': r2,\n",
    "                'mae': mae,\n",
    "                'mape': mape,\n",
    "                'mse': mse,\n",
    "                'pcc': pcc\n",
    "            }        \n",
    "            log_param_dict = {\n",
    "                \"n_layer_size\": n_neurons,\n",
    "                \"n_layers\": config['n_layers'],\n",
    "                \"n_dense_layers\": config['n_dense_layers'],\n",
    "                \"activation_fn\": config['activation'],\n",
    "                \"epochs\": n_epochs,\n",
    "                \"learning_rate\": learning_rate,\n",
    "                \"optimizer\": \"adam\",\n",
    "                \"time_window\": config['T'],\n",
    "                \"dense_dp\": config['dense_dp'],\n",
    "                \"model_exp\": model_exp            \n",
    "            }\n",
    "            exception_param_metric_dict['log_param_dict'] = log_param_dict\n",
    "            exception_param_metric_dict['log_metric_dict'] = log_metric_dict\n",
    "            mlflow.log_dict(exception_param_metric_dict, \"exception_param_metric_dict.json\")\n",
    "\n",
    "    train.report({\"mse\":mse, \"mae\":mae, \"mape\":mape, \"r2\":r2, \"autoencoder_name\":autoencoder_name, \"model_name\":model_name, \"T\":T, \"run_id\":run_id}) # for Ray>=2.7\n",
    "    \n",
    "    #air.session.report({\"mse\":mse, \"mae\":mae, \"mape\":mape, \"r2\":r2, \"autoencoder_name\":autoencoder_name, \"model_name\":model_name, \"T\":T, \"run_id\":run_id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b4d544-77f4-457f-a579-0db09ea9f872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_model(num_training_iterations, num_samples):\n",
    "    sched = AsyncHyperBandScheduler(\n",
    "        time_attr=\"training_iteration\", max_t=10, grace_period=5\n",
    "    )\n",
    "    \n",
    "    #we have a cluster of 10 worker nodes with requests 1 CPU and limits 2 CPU settings for the pod\n",
    "    resource_group = tune.PlacementGroupFactory([{'CPU': 1.0}] * 2) \n",
    "    tuner = tune.Tuner(\n",
    "        tune.with_resources(train_model, resources=resource_group),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            metric=\"mse\",\n",
    "            mode=\"min\",\n",
    "            scheduler=sched,\n",
    "            num_samples=num_samples,\n",
    "            max_concurrent_trials=10,\n",
    "        ),\n",
    "        run_config=air.RunConfig(\n",
    "            name=modelTypeName,\n",
    "            verbose = 1,\n",
    "            stop={\"training_iteration\": num_training_iterations},\n",
    "        ),\n",
    "        param_space=param_space\n",
    "    )\n",
    "    \n",
    "    results = tuner.fit()\n",
    "    return results    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425e55b9-8e9d-4a2b-9db2-1cdba6e00b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def run_n_step_evaluation(model_name, autoencoder_name, run_id, T, predict_ahead):\n",
    "    import mlflow\n",
    "\n",
    "    mlflow.set_tracking_uri(MLFLOW_URI)\n",
    "    mlflow.set_registry_uri(MLFLOW_URI)\n",
    "\n",
    "    model = mlflow.tensorflow.load_model(f'models:/{model_name}/1')\n",
    "    autoencoder = mlflow.tensorflow.load_model(f'models:/{autoencoder_name}/1')\n",
    "\n",
    "    params = mlflow.get_run(run_id).to_dictionary()['data']['params']\n",
    "    n_neurons = params['n_layer_size']\n",
    "    learning_rate = params['learning_rate']\n",
    "    n_epochs = params['epochs']\n",
    "    model_exp = params['model_exp']\n",
    "    \n",
    "    n_step_metrics = {}\n",
    "    \n",
    "    X_train, Y_train, X_test, Y_test = prepare_dataset(dataFrame[dataColumnName], T)\n",
    "    \n",
    "    #autoencode input\n",
    "    X_test_ = np.concatenate([X_test, autoencoder.predict(X_test, verbose=0)], axis = -1)\n",
    "    \n",
    "    y_predict = model.predict(X_test_, verbose=0)\n",
    "    y_pred_nsteps, avg_ae, avg_se = generate_nsteps_forecast(X_test_, Y_test, model, autoencoder, predict_ahead)\n",
    "    \n",
    "    errors_ae2 = calculate_absolute_prediction_errors(Y_test, y_pred_nsteps)\n",
    "    errors_se2 = calculate_squared_prediction_errors(Y_test, y_pred_nsteps)\n",
    "\n",
    "    try:\n",
    "        r2_nStep = r2_score(Y_test, y_pred_nsteps)\n",
    "    except:\n",
    "        r2_nStep = 100\n",
    "\n",
    "    try:\n",
    "        mae_nStep = mean_absolute_error(Y_test, y_pred_nsteps)\n",
    "    except:\n",
    "        mae_nStep = 100\n",
    "\n",
    "    try:\n",
    "        mape_nStep = mean_absolute_percentage_error(Y_test, y_pred_nsteps)\n",
    "    except:\n",
    "        mape_nStep = 100\n",
    "\n",
    "    try:\n",
    "        mse_nStep = mean_squared_error(Y_test, y_pred_nsteps)\n",
    "    except:\n",
    "        mse_nStep = 100\n",
    "\n",
    "    try:\n",
    "        pcc_nStep = np.corrcoef(Y_test, y_pred_nsteps.flatten())[0,1]\n",
    "    except:\n",
    "        pcc_nStep = 100\n",
    "\n",
    "    crt_step = f'predict_ahead_{predict_ahead}'\n",
    "    n_step_metrics[crt_step] = {\n",
    "                                    'r2_nStep': r2_nStep,\n",
    "                                    'mae_nStep': mae_nStep,\n",
    "                                    'mape_nStep': mape_nStep,\n",
    "                                    'mse_nStep': mse_nStep,\n",
    "                                    'pcc_nStep': pcc_nStep,\n",
    "                                    'agv_ae_at_nStep': avg_ae,\n",
    "                                    'agv_se_at_nStep': avg_se        \n",
    "                                   }\n",
    "\n",
    "    with mlflow.start_run(run_id=run_id, nested=True) as run:\n",
    "        artifact_uri = run.info.artifact_uri\n",
    "        three_sigma_thresholds = mlflow.artifacts.load_dict(artifact_uri + \"/three_sigma_thresholds.json\")        \n",
    "\n",
    "        anomalies_ae2 = calculate_3sigma_anomalies(errors_ae2, \n",
    "                                                   three_sigma_thresholds['lo_3sigma_ae'], \n",
    "                                                   three_sigma_thresholds['up_3sigma_ae'])\n",
    "        anomalies_se2 = calculate_3sigma_anomalies(errors_se2, \n",
    "                                                   three_sigma_thresholds['lo_3sigma_se'], \n",
    "                                                   three_sigma_thresholds['up_3sigma_se'])\n",
    "        \n",
    "        fig = plt.figure(figsize=(20,15))\n",
    "        plt.title(\"Compare forecasts T=\" + str(T) + \" predict_ahead=\" + str(predict_ahead) + \" with predict 1\" + \n",
    "                 \" for AE-LSTM \"+ modelTypeName +\": NN=\" + str(n_neurons) + \" LR= \" + str(learning_rate) + \" epochs=\" + str(n_epochs))\n",
    "        plt.plot(Y_test,label=\"Original Data\", alpha=0.6, c='red',linewidth=2)\n",
    "        plt.plot(y_predict,label=\"Predicted Data 1-step\", alpha=0.6, c='black', linewidth=2)\n",
    "        plt.plot(y_pred_nsteps,label=\"Predicted Data \" + str(predict_ahead) + \"-steps\", alpha=0.6, c='blue', linewidth=2)\n",
    "        plt.legend()\n",
    "        figName = f\"compare-forecasts-1_{predict_ahead}.png\"\n",
    "        mlflow.log_figure(fig, figName)\n",
    "        #plt.savefig(figName, transparent=False)\n",
    "        fig.clf()\n",
    "        plt.close()\n",
    "        \n",
    "        fig = plt.figure(figsize=(20,15))        \n",
    "        plt.title(\"Predict Spikes T=\" + str(T) + \" with predict \" + str(predict_ahead) + \" on \" + str(model_exp) + \": NN=\" \n",
    "                  + str(n_neurons) + \" epochs=\" + str(n_epochs) + \" lr=\" + str(learning_rate))\n",
    "        plt.plot(y_pred_nsteps,label=\"Predict \" + str(predict_ahead) + \"-step Forecast\", alpha=0.6, c='red', linewidth=3)\n",
    "        plt.plot(Y_test,label=\"Original Data\", alpha=0.6, c='black')\n",
    "        plt.scatter(np.where(anomalies_ae2==True), y_pred_nsteps[np.where(anomalies_ae2==True)], \n",
    "                    alpha=0.8, color='green', s=350, label=\"Anomalies AE\")\n",
    "        plt.scatter(np.where(anomalies_se2==True), y_pred_nsteps[np.where(anomalies_se2==True)], \n",
    "                    alpha=0.8, color='magenta', s=150, label = \"Anomalies SE\")\n",
    "        plt.legend();    \n",
    "        figName = f\"Y-predict-spikes-step-{predict_ahead}-with-T_{T}.png\"\n",
    "        mlflow.log_figure(fig, figName)\n",
    "        #plt.savefig(figName, transparent=False)\n",
    "        fig.clf()\n",
    "        plt.close()\n",
    "\n",
    "        fname = f'{predict_ahead}-step-metric.json'\n",
    "        mlflow.log_dict(n_step_metrics, fname)\n",
    "        \n",
    "    return n_step_metrics\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3685995a-dc84-4e20-8573-c47aee320e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# Set the target environment to cluster or local. Default is local\n",
    "####\n",
    "run_local = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71acccd1-bb3d-4a44-8e3b-6076d2b4e51a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if run_local == False:\n",
    "    print(\"Running on Cluster\")\n",
    "    # Run this cell only if you are using RHOAI as training environment. Otherwise run only the next one\n",
    "    from codeflare_sdk import TokenAuthentication\n",
    "    from codeflare_sdk import generate_cert\n",
    "    \n",
    "    auth = TokenAuthentication(\n",
    "        token = \"<your_access_token>\", # execute ocp whoami -t on the authenticated cluster to obtain the token\n",
    "        server = \"<OCP cluster API URL>\",\n",
    "        skip_tls = True\n",
    "    )\n",
    "    auth.login()\n",
    "    \n",
    "    # Create required TLS cert and export the environment variables to enable TLS\n",
    "    generate_cert.generate_tls_cert('raycluster-complete', 'raycluster')\n",
    "    generate_cert.export_env('raycluster-complete', 'raycluster')\n",
    "    \n",
    "    ray_endpoint = 'ray://raycluster-complete-head-svc.raycluster.svc.cluster.local:10001' # ensure your ray cluster URL is correct\n",
    "    ray.shutdown()\n",
    "    ray.init(address=ray_endpoint, logging_level=logging.ERROR, log_to_driver=False)\n",
    "else:\n",
    "    ray.shutdown()\n",
    "    ray.init()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b6c961-0a45-499e-8246-3ac8e2cc83ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lstm_ae = tune_model(1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb745e82-4ebb-40ed-84fd-1df14fc2cced",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = results_lstm_ae.get_best_result()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dda6bfd-bfb2-4bd3-86b8-2df6ebb0fcd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_n_step_predict = {}\n",
    "#for result in results_lstm_ae:\n",
    "try:\n",
    "    model_name = result.metrics['model_name']\n",
    "    autoencoder_name = result.metrics['autoencoder_name']\n",
    "    T = result.metrics['T']\n",
    "    run_id = result.metrics['run_id']\n",
    "    for predict_ahead in [5, 10, 15, 30, 45, 60, 90, 120]: \n",
    "    #for predict_ahead in [5, 10]: \n",
    "        res = run_n_step_evaluation.remote(model_name, autoencoder_name, run_id, T, predict_ahead)\n",
    "        #res = run_n_step_evaluation(model_name, autoencoder_name, run_id, T, predict_ahead)\n",
    "        tag = f'runID_{run_id}-model_{model_name}-T_{T}-p_ahead_{predict_ahead}'\n",
    "        results_n_step_predict[tag] = res\n",
    "        print(f'Scheduled job for T:{T}, predict_ahead: {predict_ahead}, model_name:{model_name}, run_id:{run_id}')    \n",
    "except:\n",
    "    print(f'ERROR scheduling job for T:{T}, predict_ahead: {predict_ahead}, model_name:{model_name}, run_id:{run_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbb6f34-c6cb-41de-9f04-cab69d5fdc11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ex_n_step = {}\n",
    "for item in results_n_step_predict.keys():\n",
    "    try:\n",
    "        res = ray.get(results_n_step_predict[item])\n",
    "    except:\n",
    "        print(f'Error getting results for key:{item}')\n",
    "        res = None\n",
    "        \n",
    "    ex_n_step[item]=res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f1feed-4cf1-404f-b3f8-8d952d643a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ex_n_step.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcacf9d-2008-477d-a887-7d38290cec72",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_n_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ff61e3-8f4c-43e4-866c-24d6862940dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce80a94d-4a12-434f-822d-832a8eb9ad55",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b783e8-43cf-425e-ba91-117a37320ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_difference = (end_time - start_time).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b65e03-66b1-44bd-96dc-9190d1f0579a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Experiment runtime was {time_difference} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e4843f-6f70-4278-b604-83412eccb82e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF-CPU",
   "language": "python",
   "name": "tf-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
